{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
      "metadata": {
        "interpreter": {
          "hash": "c025893ad393b6c1470ef4b4e4388f7df2327cf9c3b12a2f0dae4fc3e3221f44"
        }
      }
    },
    "colab": {
      "name": "hw5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smqwgw54Hc-D"
      },
      "source": [
        "# 1. Time series gene regulatory network inference\n",
        "We want to infer the gene regulatory network (denote with adjacency matrix $\\mathbf{A}$) given the time-series gene expression data $x$. The gene expression data records the change of $10$ gene expression values from time point $1$ to $50$. \n",
        "\n",
        "In the lecture, we talked about multiple ways to infer the gene regulatory network with data driven methods, one of the most straightforward ways is to use the differential equation model. Let's assume the gene expression data follows:\n",
        "\n",
        "$$\n",
        "\\frac{d\\mathbf{x}(t)}{dt} = \\mathbf{Ax}(t) + \\mathbf{n},\\, \\mathbf{n}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})\n",
        "$$\n",
        "\n",
        "We can estimate the changing speed of gene expression data $\\frac{d\\mathbf{x}(t)}{dt}$ at time point $t$($t = 1,2,3,\\cdots,49$) with $\\Delta \\mathbf{x}(t) = \\mathbf{x}(t+1) - \\mathbf{x}(t)$. And the problem turns into a [least square](https://en.wikipedia.org/wiki/Least_squares#Lasso_method) problem\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\arg\\min_{\\mathbf{A}}\\sum_{t = 1}^{49}\\Vert\\Delta \\mathbf{x}(t) - \\mathbf{Ax}(t)\\Vert_2^2 \n",
        "$$\n",
        "\n",
        "Please find the solution of the least square problem above using python. \n",
        "\n",
        "(Hint: You need to\n",
        "* Calculate $\\Delta \\mathbf{x}(t)$ using $\\mathbf{x}(t)$\n",
        "* Calculate optimal $\\mathbf{A}$ using [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,the%20direction%20of%20steepest%20descent), or by setting the gradient of the objective function(w.r.t $\\mathbf{A}$) to be $0$. \n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LfhiP3GHc-E"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "HpO1VrouHc-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638c99e5-7fdf-4922-d86a-4c5e3b42036f"
      },
      "source": [
        "# xs stores the time series data, it is of the dimension (10,50), correspond to the expression data of 10 genes across 50 time points\n",
        "xs = np.load(\"counts.npy\")\n",
        "# TODO: implement your code here\n",
        "\n",
        "# delt_x\n",
        "xs_del_0 = np.delete(xs, 0, 1)\n",
        "xs_del_49 = np.delete(xs, 49, 1)\n",
        "delt_x = np.subtract(xs_del_0, xs_del_49)\n",
        "\n",
        "\n",
        "def gradient(A_cal):\n",
        "    return -2 * np.dot(delt_x - np.dot(A_cal,xs_del_49), xs_del_49.T) \n",
        "\n",
        "\n",
        "def gradient_descent(\n",
        "    gradient, start, learn_rate=0.0001, n_iter=200, tolerance=1e-06\n",
        "):\n",
        "    A_cal = start\n",
        "    A_history = A_cal\n",
        "    f_history = delt_x - np.dot(A_cal,xs_del_49)\n",
        "    for _ in range(n_iter):\n",
        "        diff = -learn_rate * gradient(A_cal)\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "        A_cal += diff\n",
        "        A_history = np.vstack((A_history,A_cal))\n",
        "        f_history = np.vstack((f_history,delt_x - np.dot(A_cal,xs_del_49)))\n",
        "    return A_cal, A_history, f_history\n",
        "\n",
        "A1, A1_history, f1_history = gradient_descent(gradient,np.zeros((10,10)))\n",
        "A1"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.54244002e-04, 1.46574887e-05, 2.66614023e-04, 9.05815137e-05,\n",
              "        2.13589287e-04, 7.04775028e-05, 2.65738754e-04, 1.32969240e-04,\n",
              "        9.96570725e-05, 2.16704979e-04],\n",
              "       [1.25119379e-04, 1.20106117e-05, 2.16365424e-04, 7.33864786e-05,\n",
              "        1.73078891e-04, 5.73989698e-05, 2.15531030e-04, 1.07841564e-04,\n",
              "        8.08221837e-05, 1.75829628e-04],\n",
              "       [3.52910619e-04, 3.40029031e-05, 6.10376366e-04, 2.06898157e-04,\n",
              "        4.87997133e-04, 1.62137899e-04, 6.07893236e-04, 3.04155943e-04,\n",
              "        2.27948181e-04, 4.95988640e-04],\n",
              "       [7.30606801e-06, 8.04356101e-07, 1.27143140e-05, 4.20738904e-06,\n",
              "        9.95294508e-06, 3.54716372e-06, 1.25594819e-05, 6.27995005e-06,\n",
              "        4.70454364e-06, 1.03040954e-05],\n",
              "       [4.91025546e-05, 4.70454364e-06, 8.49047163e-05, 2.88069944e-05,\n",
              "        6.79374434e-05, 2.25089745e-05, 8.45865013e-05, 4.23234583e-05,\n",
              "        3.17196291e-05, 6.90003776e-05],\n",
              "       [2.80301706e-04, 2.71046882e-05, 4.84871563e-04, 1.64256479e-04,\n",
              "        3.87449481e-04, 1.28964394e-04, 4.82798744e-04, 2.41561631e-04,\n",
              "        1.81035273e-04, 3.93977364e-04],\n",
              "       [2.38289958e-04, 2.30184094e-05, 4.12180304e-04, 1.39655660e-04,\n",
              "        3.29413908e-04, 1.09589988e-04, 4.10442693e-04, 2.05360255e-04,\n",
              "        1.53905094e-04, 3.34919353e-04],\n",
              "       [1.14620661e-04, 1.09844937e-05, 1.98196062e-04, 6.72424965e-05,\n",
              "        1.58583225e-04, 5.25479112e-05, 1.97450544e-04, 9.87956876e-05,\n",
              "        7.40430874e-05, 1.61069287e-04],\n",
              "       [8.39925499e-05, 8.25170737e-06, 1.45393015e-04, 4.91215041e-05,\n",
              "        1.15905972e-04, 3.88905200e-05, 1.44638237e-04, 7.23623950e-05,\n",
              "        5.42286036e-05, 1.18102066e-04],\n",
              "       [2.56580086e-04, 2.47249257e-05, 4.43770620e-04, 1.50420583e-04,\n",
              "        3.54788198e-04, 1.17887268e-04, 4.41961721e-04, 2.21132910e-04,\n",
              "        1.65726900e-04, 3.60604736e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjy0NcPXHc-F"
      },
      "source": [
        "# 2. Incorporate l1 term \n",
        "\n",
        "\n",
        "Gene regulatory network tend to be a sparse graph(have a very small number of $1$s), which can be learned with an $\\ell 1$ regularization term. By adding the $\\ell 1$ regularization term into the objective function above, the problem turns into a LASSO regression problem:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\arg\\min_{\\mathbf{A}}\\sum_{t = 1}^{49}\\Vert\\Delta \\mathbf{x}(t) - \\mathbf{Ax}(t)\\Vert_2^2 + \\lambda * \\Vert\\mathbf{A}\\Vert_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Vert\\mathbf{A}\\Vert_1 = \\sum_i\\sum_j| A_{ij}|_1\n",
        "$$\n",
        "\n",
        "It is hard to solve a lasso regression problem using traditional gradient descent algorithm, since $\\ell 1$ regularization is non-differentiable. **Iterative soft thresholding algorithm** (**ISTA**, a proximal gradient descent algorithm) is a good choice for solving such problem. Please implement ISTA algorithm in the block below.\n",
        "\n",
        "* Information about ISTA algorithm: https://www.stat.cmu.edu/~ryantibs/convexopt-F15/lectures/08-prox-grad.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFw7XXMDHc-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7581d4-e8cd-41fa-9c50-070e4e8664a3"
      },
      "source": [
        "xs = np.load(\"counts.npy\")\n",
        "# TODO: implement your code here\n",
        "\n",
        "def  soft_thresholding_operator(A_ij, Lamb):\n",
        "  if A_ij > Lamb:\n",
        "    return A_ij - Lamb\n",
        "  elif A_ij < -Lamb:\n",
        "    return A_ij + Lamb\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "vfunc = np.vectorize(soft_thresholding_operator)\n",
        "\n",
        "def ISTA(start, learn_rate=0.0001, n_iter=300, Lamb = 2.8e-6 #, tolerance=1e-05\n",
        "         ):\n",
        "    A_cal = start\n",
        "   # A_history = A_cal\n",
        "    #f_history = delt_x - np.dot(A_cal,xs_del_49)\n",
        "    for _ in range(n_iter):\n",
        "        A_cal = A_cal + learn_rate * np.dot(delt_x - np.dot(A_cal,xs_del_49), xs_del_49.T) \n",
        "        #for each element in A_cal, apply soft-thresholding operator here\n",
        "        A_cal = vfunc(A_cal, Lamb)\n",
        "        #if np.all(np.abs(delt_x - np.dot(A_cal,xs_del_49)) <= tolerance):\n",
        "        #    break\n",
        "        #A_history = np.vstack((A_history,A_cal))\n",
        "        #f_history = np.vstack((f_history,delt_x - np.dot(A_cal,xs_del_49)))\n",
        "    return A_cal #, A_history, f_history\n",
        "\n",
        "\n",
        "A2= ISTA(np.zeros((10,10)))\n",
        "A2"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.30728736e-09, 0.00000000e+00, 1.22576590e-05, 0.00000000e+00,\n",
              "        6.45325315e-06, 0.00000000e+00, 1.21499415e-05, 0.00000000e+00,\n",
              "        0.00000000e+00, 6.81489963e-06],\n",
              "       [0.00000000e+00, 0.00000000e+00, 6.89173333e-06, 0.00000000e+00,\n",
              "        2.15034774e-06, 0.00000000e+00, 6.79960717e-06, 0.00000000e+00,\n",
              "        0.00000000e+00, 2.45291335e-06],\n",
              "       [2.10347312e-05, 0.00000000e+00, 4.86315148e-05, 5.38643681e-06,\n",
              "        3.55276286e-05, 5.68957691e-07, 4.83726161e-05, 1.58090412e-05,\n",
              "        7.63882211e-06, 3.63712500e-05],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [1.33615132e-05, 0.00000000e+00, 3.53640316e-05, 8.84296322e-07,\n",
              "        2.49101219e-05, 0.00000000e+00, 3.51541860e-05, 9.19507834e-06,\n",
              "        2.68192695e-06, 2.55888504e-05],\n",
              "       [8.91797664e-06, 0.00000000e+00, 2.76783277e-05, 0.00000000e+00,\n",
              "        1.87664437e-05, 0.00000000e+00, 2.75002633e-05, 5.36550151e-06,\n",
              "        0.00000000e+00, 1.93436247e-05],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.95554937e-06, 0.00000000e+00,\n",
              "        6.02983978e-07, 0.00000000e+00, 4.87160103e-06, 0.00000000e+00,\n",
              "        0.00000000e+00, 8.79660632e-07],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [1.08536620e-05, 0.00000000e+00, 3.10244149e-05, 0.00000000e+00,\n",
              "        2.14464712e-05, 0.00000000e+00, 3.08350821e-05, 7.03415294e-06,\n",
              "        1.06248628e-06, 2.20632627e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIK9CVeuHc-F"
      },
      "source": [
        "# 3. Benchmark\n",
        "Calculate the difference between the ground truth gene regulatory network(denote as `A`) and the inferred gene regulatory network (step 1 result denoted as `A1`, step 2 result denoted as `A2`) using the `diff` function below. Adjust the hyper-parameter within the algorithms in step $1$ and $2$, and report the best result you can obtain(The smaller the difference, the better the result is)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abj520iWHc-G"
      },
      "source": [
        "A = np.load(\"gt_grn.npy\")\n",
        "\n",
        "def diff(A_inf, A):\n",
        "    A_inf = (A_inf > 2e-05).astype(np.int)\n",
        "    return np.sum(np.abs(A_inf - A))/(A.shape[0] * A.shape[1])\n",
        "    "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Ooa1ddHc-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97c3ea4-e043-4060-bad0-9ea7058f5376"
      },
      "source": [
        "# step1\n",
        "diff(A1, A)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8BzTJFRHc-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f8fe01-8f9d-4b86-ae92-ef3b824a3b73"
      },
      "source": [
        "# step2\n",
        "diff(A2, A)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X466JMzPHc-G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}